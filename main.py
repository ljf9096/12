import urllib.request
from urllib.parse import urlparse
import re
import os
from datetime import datetime, timedelta, timezone
import random
import opencc
from typing import List, Set, Dict, Tuple
from collections import defaultdict

class TVChannelProcessor:
    def __init__(self):
        self.timestart = datetime.now()
        self.combined_blacklist = set()
        self.all_urls = set()
        
        # åˆå§‹åŒ–é¢‘é“å®¹å™¨ - åªä¿ç•™ä¸‰ä¸ªåˆ†ç±»
        self.ys_lines = []  # å¤®è§†é¢‘é“
        self.ws_lines = []  # å«è§†é¢‘é“
        self.newtv_lines = []  # NewTVé¢‘é“
        
        # å­˜å‚¨æ¯ä¸ªé¢‘é“çš„URLå’Œå“åº”æ—¶é—´
        self.channel_data = defaultdict(list)
        
        self.removal_list = ["ã€ŒIPV4ã€","ã€ŒIPV6ã€","[ipv6]","[ipv4]","_ç”µä¿¡", "ç”µä¿¡","ï¼ˆHDï¼‰","[è¶…æ¸…]","é«˜æ¸…","è¶…æ¸…", "-HD","(HK)","AKtv","@","IPV6","ğŸï¸","ğŸ¦"," ","[BD]","[VGA]","[HD]","[SD]","(1080p)","(720p)","(480p)"]

    def read_txt_to_array(self, file_name: str) -> List[str]:
        """è¯»å–æ–‡æœ¬æ–‡ä»¶åˆ°æ•°ç»„"""
        try:
            with open(file_name, 'r', encoding='utf-8') as file:
                return [line.strip() for line in file if line.strip()]
        except FileNotFoundError:
            print(f"æ–‡ä»¶æœªæ‰¾åˆ°: {file_name}")
            return []
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶é”™è¯¯ {file_name}: {e}")
            return []

    def read_blacklist_from_txt(self, file_path: str) -> List[str]:
        """è¯»å–é»‘åå•"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                return [line.split(',')[1].strip() for line in file if ',' in line]
        except Exception as e:
            print(f"è¯»å–é»‘åå•é”™è¯¯ {file_path}: {e}")
            return []

    def load_corrections_name(self, filename: str) -> Dict[str, str]:
        """åŠ è½½é¢‘é“åç§°ä¿®æ­£"""
        corrections = {}
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        parts = line.strip().split(',')
                        if len(parts) >= 2:
                            correct_name = parts[0]
                            for name in parts[1:]:
                                corrections[name] = correct_name
        except Exception as e:
            print(f"åŠ è½½ä¿®æ­£æ–‡ä»¶é”™è¯¯: {e}")
        return corrections

    def traditional_to_simplified(self, text: str) -> str:
        """ç¹ä½“è½¬ç®€ä½“"""
        try:
            converter = opencc.OpenCC('t2s')
            return converter.convert(text)
        except Exception as e:
            print(f"ç¹ç®€è½¬æ¢é”™è¯¯: {e}")
            return text

    def clean_channel_name(self, channel_name: str) -> str:
        """æ¸…ç†é¢‘é“åç§°"""
        for item in self.removal_list:
            channel_name = channel_name.replace(item, "")
        
        replacements = {
            "CCTV-": "CCTV",
            "CCTV0": "CCTV",
            "PLUS": "+",
            "NewTV-": "NewTV",
            "iHOT-": "iHOT",
            "NEW": "New",
            "New_": "New"
        }
        
        for old, new in replacements.items():
            channel_name = channel_name.replace(old, new)
            
        return channel_name.strip()

    def clean_url(self, url: str) -> str:
        """æ¸…ç†URL"""
        last_dollar_index = url.rfind('$')
        return url[:last_dollar_index] if last_dollar_index != -1 else url

    def add_channel_url(self, channel_name: str, channel_url: str, response_time: float = 9999.0):
        """æ·»åŠ é¢‘é“URLåˆ°å¯¹åº”åˆ†ç±»ï¼Œåªä¿ç•™æœ€å¿«çš„å‰5ä¸ª"""
        if not channel_url or channel_url in self.combined_blacklist:
            return
            
        # å¦‚æœURLå·²å­˜åœ¨ï¼Œè·³è¿‡
        if channel_url in self.all_urls:
            return
            
        self.all_urls.add(channel_url)
        
        # å­˜å‚¨é¢‘é“æ•°æ®ï¼ŒåŒ…å«å“åº”æ—¶é—´
        self.channel_data[channel_name].append((response_time, channel_url))
        
        # æ¯ä¸ªé¢‘é“åªä¿ç•™æœ€å¿«çš„å‰5ä¸ªURL
        if len(self.channel_data[channel_name]) > 5:
            # æŒ‰å“åº”æ—¶é—´æ’åºï¼Œä¿ç•™æœ€å¿«çš„5ä¸ª
            self.channel_data[channel_name].sort(key=lambda x: x[0])
            self.channel_data[channel_name] = self.channel_data[channel_name][:5]

    def process_line(self, line: str):
        """å¤„ç†å•è¡Œæ•°æ®"""
        if "#genre#" in line or "#EXTINF:" in line or "://" not in line:
            return
            
        try:
            parts = line.split(',', 1)
            if len(parts) != 2:
                return
                
            channel_name, channel_url = parts
            channel_name = self.traditional_to_simplified(channel_name)
            channel_name = self.clean_channel_name(channel_name)
            channel_name = self.corrections_name.get(channel_name, channel_name)
            
            channel_url = self.clean_url(channel_url.strip())
            
            # å¤„ç†åŒ…å«å¤šä¸ªURLçš„æƒ…å†µ
            if '#' in channel_url:
                urls = channel_url.split('#')
                for url in urls:
                    if url.strip() and '://' in url:
                        self.add_channel_url(channel_name, url.strip())
            else:
                if channel_url.strip() and '://' in channel_url:
                    self.add_channel_url(channel_name, channel_url.strip())
                    
        except Exception as e:
            print(f"å¤„ç†è¡Œé”™è¯¯: {line}, é”™è¯¯: {e}")

    def process_whitelist_line(self, line: str):
        """å¤„ç†ç™½åå•è¡Œï¼ˆåŒ…å«å“åº”æ—¶é—´ï¼‰"""
        if "#genre#" in line or "://" not in line:
            return
            
        try:
            # ç™½åå•æ ¼å¼: å“åº”æ—¶é—´,é¢‘é“åç§°,URL
            parts = line.split(',', 2)
            if len(parts) == 3:
                response_time_str, channel_name, channel_url = parts
                try:
                    response_time = float(response_time_str.replace("ms", "").strip())
                    if response_time >= 2000:  # è¶…è¿‡2ç§’çš„è·³è¿‡
                        return
                except ValueError:
                    response_time = 9999.0
                
                channel_name = self.traditional_to_simplified(channel_name)
                channel_name = self.clean_channel_name(channel_name)
                channel_name = self.corrections_name.get(channel_name, channel_name)
                
                channel_url = self.clean_url(channel_url.strip())
                
                if channel_url and '://' in channel_url:
                    self.add_channel_url(channel_name, channel_url, response_time)
                    
        except Exception as e:
            print(f"å¤„ç†ç™½åå•è¡Œé”™è¯¯: {line}, é”™è¯¯: {e}")

    def download_and_process_url(self, url: str):
        """ä¸‹è½½å¹¶å¤„ç†URLå†…å®¹"""
        print(f"å¤„ç†URL: {url}")
        
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            req = urllib.request.Request(url, headers=headers)
            
            with urllib.request.urlopen(req, timeout=15) as response:
                content = response.read()
                
                # å°è¯•ä¸åŒç¼–ç 
                encodings = ['utf-8', 'gbk', 'gb2312', 'iso-8859-1']
                text = None
                
                for encoding in encodings:
                    try:
                        text = content.decode(encoding)
                        break
                    except UnicodeDecodeError:
                        continue
                
                if text is None:
                    print(f"æ— æ³•è§£ç å†…å®¹: {url}")
                    return
                
                # å¤„ç†M3Uæ ¼å¼
                if text.strip().startswith("#EXTM3U"):
                    text = self.convert_m3u_to_txt(text)
                
                # å¤„ç†æ¯ä¸€è¡Œ
                for line in text.splitlines():
                    line = line.strip()
                    if line:
                        self.process_line(line)
                        
        except Exception as e:
            print(f"å¤„ç†URLé”™è¯¯ {url}: {e}")

    def convert_m3u_to_txt(self, m3u_content: str) -> str:
        """è½¬æ¢M3Uæ ¼å¼ä¸ºTXTæ ¼å¼"""
        lines = m3u_content.split('\n')
        result = []
        current_name = ""
        
        for line in lines:
            line = line.strip()
            if line.startswith("#EXTINF"):
                # æå–é¢‘é“åç§°
                parts = line.split(',', 1)
                if len(parts) > 1:
                    current_name = parts[1]
            elif line.startswith(("http://", "https://", "rtmp://", "rtsp://")):
                if current_name:
                    result.append(f"{current_name},{line}")
                    current_name = ""
        
        return '\n'.join(result)

    def is_ys_channel(self, channel_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¤®è§†é¢‘é“"""
        ys_keywords = ['CCTV', 'å¤®è§†', 'ä¸­å¤®']
        return any(keyword in channel_name for keyword in ys_keywords)

    def is_ws_channel(self, channel_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå«è§†é¢‘é“"""
        ws_keywords = [
            'å«è§†', 'æ¹–å—', 'æµ™æ±Ÿ', 'æ±Ÿè‹', 'åŒ—äº¬', 'ä¸œæ–¹', 'å¹¿ä¸œ', 'æ·±åœ³', 
            'å¤©æ´¥', 'é‡åº†', 'å±±ä¸œ', 'æ¹–åŒ—', 'å››å·', 'è¾½å®', 'æ²³å—', 'å®‰å¾½',
            'æ²³åŒ—', 'ç¦å»º', 'æ±Ÿè¥¿', 'å¹¿è¥¿', 'è´µå·', 'é»‘é¾™æ±Ÿ', 'å‰æ—', 'å±±è¥¿',
            'é™•è¥¿', 'äº‘å—', 'æµ·å—', 'ç”˜è‚ƒ', 'å®å¤', 'é’æµ·', 'è¥¿è—', 'æ–°ç–†',
            'å†…è’™å¤', 'å‡¤å‡°', 'ç¿¡ç¿ '
        ]
        return any(keyword in channel_name for keyword in ws_keywords)

    def is_newtv_channel(self, channel_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºNewTVé¢‘é“"""
        newtv_keywords = ['NewTV', 'New TV', 'NEWTV']
        return any(keyword.upper() in channel_name.upper() for keyword in newtv_keywords)

    def categorize_channels(self):
        """å°†é¢‘é“åˆ†ç±»åˆ°å¯¹åº”çš„å®¹å™¨ä¸­"""
        for channel_name, url_list in self.channel_data.items():
            # æŒ‰å“åº”æ—¶é—´æ’åºï¼Œå–æœ€å¿«çš„å‰5ä¸ª
            url_list.sort(key=lambda x: x[0])
            
            for response_time, url in url_list[:5]:
                line = f"{channel_name},{url}"
                
                # åˆ†ç±»é€»è¾‘ - åªä¿ç•™ä¸‰ä¸ªåˆ†ç±»
                if self.is_ys_channel(channel_name):
                    self.ys_lines.append(line)
                elif self.is_ws_channel(channel_name):
                    self.ws_lines.append(line)
                elif self.is_newtv_channel(channel_name):
                    self.newtv_lines.append(line)
                # å…¶ä»–é¢‘é“ç›´æ¥å¿½ç•¥ï¼Œä¸æ·»åŠ åˆ°ä»»ä½•åˆ†ç±»

    def generate_output_files(self):
        """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶"""
        # è·å–å½“å‰æ—¶é—´
        beijing_time = datetime.now(timezone.utc) + timedelta(hours=8)
        formatted_time = beijing_time.strftime("%Y%m%d %H:%M")
        
        # æ„å»ºlive.txtå†…å®¹
        content_lines = []
        content_lines.append("æ›´æ–°æ—¶é—´,#genre#")
        content_lines.append(formatted_time)
        content_lines.append("")
        
        # æ·»åŠ ä¸‰ä¸ªåˆ†ç±»
        categories = [
            ("å¤®è§†é¢‘é“,#genre#", self.ys_lines),
            ("å«è§†é¢‘é“,#genre#", self.ws_lines),
            ("NewTVé¢‘é“,#genre#", self.newtv_lines),
        ]
        
        for category_name, lines in categories:
            if lines:  # åªæ·»åŠ æœ‰å†…å®¹çš„åˆ†ç±»
                content_lines.append(category_name)
                content_lines.extend(lines)
                content_lines.append("")
        
        # å†™å…¥live.txt
        try:
            with open("live.txt", 'w', encoding='utf-8') as f:
                f.write('\n'.join(content_lines))
            print("live.txt ç”ŸæˆæˆåŠŸ")
            print(f"å¤®è§†é¢‘é“æ•°é‡: {len(self.ys_lines)}")
            print(f"å«è§†é¢‘é“æ•°é‡: {len(self.ws_lines)}")
            print(f"NewTVé¢‘é“æ•°é‡: {len(self.newtv_lines)}")
        except Exception as e:
            print(f"å†™å…¥live.txté”™è¯¯: {e}")

    def make_m3u(self, txt_file: str, m3u_file: str):
        """ç”ŸæˆM3Uæ–‡ä»¶"""
        try:
            m3u_content = ['#EXTM3U x-tvg-url="https://epg.112114.xyz/pp.xml.gz"']
            
            with open(txt_file, 'r', encoding='utf-8') as f:
                current_group = ""
                
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    
                    if line.endswith("#genre#"):
                        current_group = line.replace(",#genre#", "")
                    elif ',' in line and '://' in line:
                        channel_name, url = line.split(',', 1)
                        logo_url = f"https://epg.112114.xyz/logo/{channel_name}.png"
                        
                        m3u_content.append(f'#EXTINF:-1 tvg-name="{channel_name}" tvg-logo="{logo_url}" group-title="{current_group}",{channel_name}')
                        m3u_content.append(url)
            
            with open(m3u_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(m3u_content))
                
            print(f"{m3u_file} ç”ŸæˆæˆåŠŸ")
            
        except Exception as e:
            print(f"ç”ŸæˆM3Uæ–‡ä»¶é”™è¯¯: {e}")

    def run(self):
        """ä¸»è¿è¡Œå‡½æ•°"""
        print("å¼€å§‹å¤„ç†ç”µè§†é¢‘é“...")
        print("åªä¿ç•™ï¼šå¤®è§†é¢‘é“ã€å«è§†é¢‘é“ã€NewTVé¢‘é“")
        print("æ¯ä¸ªé¢‘é“åªä¿ç•™å“åº”æ—¶é—´æœ€å¿«çš„å‰5ä¸ªæº")
        
        # åŠ è½½é»‘åå•
        blacklist_auto = self.read_blacklist_from_txt('assets/whitelist-blacklist/blacklist_auto.txt')
        blacklist_manual = self.read_blacklist_from_txt('assets/whitelist-blacklist/blacklist_manual.txt')
        self.combined_blacklist = set(blacklist_auto + blacklist_manual)
        
        # åŠ è½½ç™½åå•
        whitelist_manual = self.read_txt_to_array('assets/whitelist-blacklist/whitelist_manual.txt')
        whitelist_auto = self.read_txt_to_array('assets/whitelist-blacklist/whitelist_auto.txt')
        
        # åŠ è½½åç§°ä¿®æ­£
        self.corrections_name = self.load_corrections_name('assets/corrections_name.txt')
        
        # å¤„ç†æ‰‹åŠ¨ç™½åå•
        print("å¤„ç†æ‰‹åŠ¨ç™½åå•...")
        for line in whitelist_manual:
            self.process_line(line)
        
        # å¤„ç†è‡ªåŠ¨ç™½åå•ï¼ˆå«å“åº”æ—¶é—´ï¼‰
        print("å¤„ç†è‡ªåŠ¨ç™½åå•...")
        for line in whitelist_auto:
            self.process_whitelist_line(line)
        
        # å¤„ç†URLåˆ—è¡¨
        urls = self.read_txt_to_array('assets/urls.txt')
        print(f"å‘ç° {len(urls)} ä¸ªURLéœ€è¦å¤„ç†")
        
        for url in urls:
            if url.startswith('http'):
                self.download_and_process_url(url)
        
        # åˆ†ç±»é¢‘é“
        print("åˆ†ç±»é¢‘é“...")
        self.categorize_channels()
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
        print("ç”Ÿæˆè¾“å‡ºæ–‡ä»¶...")
        self.generate_output_files()
        self.make_m3u("live.txt", "live.m3u")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.print_statistics()

    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        timeend = datetime.now()
        elapsed = timeend - self.timestart
        total_seconds = elapsed.total_seconds()
        minutes = int(total_seconds // 60)
        seconds = int(total_seconds % 60)
        
        total_channels = len(self.ys_lines) + len(self.ws_lines) + len(self.newtv_lines)
        
        print(f"\n=== å¤„ç†å®Œæˆ ===")
        print(f"æ‰§è¡Œæ—¶é—´: {minutes}åˆ†{seconds}ç§’")
        print(f"é»‘åå•æ•°é‡: {len(self.combined_blacklist)}")
        print(f"å¤®è§†é¢‘é“æ•°: {len(self.ys_lines)}")
        print(f"å«è§†é¢‘é“æ•°: {len(self.ws_lines)}")
        print(f"NewTVé¢‘é“æ•°: {len(self.newtv_lines)}")
        print(f"æ€»é¢‘é“æ•°: {total_channels}")
        print(f"æ¯ä¸ªé¢‘é“ä¿ç•™æœ€å¿«çš„å‰5ä¸ªæº")

if __name__ == "__main__":
    processor = TVChannelProcessor()
    processor.run()
